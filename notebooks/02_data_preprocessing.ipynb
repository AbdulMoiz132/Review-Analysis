{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd4e7639",
   "metadata": {},
   "source": [
    "# Step 2: Data Preprocessing and Text Cleaning\n",
    "\n",
    "This notebook covers the second step of our sentiment analysis project:\n",
    "- Loading the dataset from Step 1\n",
    "- Text cleaning and preprocessing\n",
    "- Removing stopwords, punctuation, and special characters\n",
    "- Converting text to lowercase\n",
    "- Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b7a59",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b6e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For text processing\n",
    "import re\n",
    "import string\n",
    "\n",
    "# NLTK for natural language processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2019a78d",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fbbc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset created in Step 1\n",
    "df = pd.read_csv('../data/imdb_reviews.csv')\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93115bd",
   "metadata": {},
   "source": [
    "## Text Preprocessing Functions\n",
    "\n",
    "Let's create functions to clean and preprocess the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea0056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer and stopwords\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Remove stopwords from text\n",
    "    \"\"\"\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "def stem_text(text):\n",
    "    \"\"\"\n",
    "    Apply stemming to text\n",
    "    \"\"\"\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stemmed_text = [stemmer.stem(word) for word in word_tokens]\n",
    "    return ' '.join(stemmed_text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Complete text preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # Clean text\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = remove_stopwords(text)\n",
    "    \n",
    "    # Apply stemming\n",
    "    text = stem_text(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"Text preprocessing functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a892eef6",
   "metadata": {},
   "source": [
    "## Test Preprocessing Functions\n",
    "\n",
    "Let's test our preprocessing functions on sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b227a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test preprocessing on a sample review\n",
    "sample_text = \"This movie was absolutely fantastic! The acting was superb and the plot was engaging throughout.\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nAfter cleaning:\")\n",
    "cleaned = clean_text(sample_text)\n",
    "print(cleaned)\n",
    "print(\"\\nAfter removing stopwords:\")\n",
    "no_stopwords = remove_stopwords(cleaned)\n",
    "print(no_stopwords)\n",
    "print(\"\\nAfter stemming:\")\n",
    "stemmed = stem_text(no_stopwords)\n",
    "print(stemmed)\n",
    "print(\"\\nComplete preprocessing:\")\n",
    "preprocessed = preprocess_text(sample_text)\n",
    "print(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f09918d",
   "metadata": {},
   "source": [
    "## Apply Preprocessing to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51da2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to all reviews\n",
    "print(\"Applying preprocessing to all reviews...\")\n",
    "\n",
    "# Create a copy of the dataframe\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Apply preprocessing\n",
    "df_processed['cleaned_review'] = df_processed['review'].apply(preprocess_text)\n",
    "\n",
    "print(\"Preprocessing completed!\")\n",
    "print(f\"\\nDataset shape: {df_processed.shape}\")\n",
    "print(f\"New columns: {df_processed.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1325d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs cleaned reviews\n",
    "print(\"Comparison of Original vs Cleaned Reviews:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {df_processed.iloc[i]['review']}\")\n",
    "    print(f\"Cleaned:  {df_processed.iloc[i]['cleaned_review']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3916db9",
   "metadata": {},
   "source": [
    "## Text Statistics After Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f05323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text statistics\n",
    "df_processed['original_length'] = df_processed['review'].str.len()\n",
    "df_processed['cleaned_length'] = df_processed['cleaned_review'].str.len()\n",
    "df_processed['original_words'] = df_processed['review'].str.split().str.len()\n",
    "df_processed['cleaned_words'] = df_processed['cleaned_review'].str.split().str.len()\n",
    "\n",
    "print(\"Text Statistics Summary:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Average original text length: {df_processed['original_length'].mean():.1f} characters\")\n",
    "print(f\"Average cleaned text length: {df_processed['cleaned_length'].mean():.1f} characters\")\n",
    "print(f\"Average original word count: {df_processed['original_words'].mean():.1f} words\")\n",
    "print(f\"Average cleaned word count: {df_processed['cleaned_words'].mean():.1f} words\")\n",
    "\n",
    "# Reduction in text size\n",
    "char_reduction = (1 - df_processed['cleaned_length'].mean() / df_processed['original_length'].mean()) * 100\n",
    "word_reduction = (1 - df_processed['cleaned_words'].mean() / df_processed['original_words'].mean()) * 100\n",
    "\n",
    "print(f\"\\nReduction after preprocessing:\")\n",
    "print(f\"Character reduction: {char_reduction:.1f}%\")\n",
    "print(f\"Word reduction: {word_reduction:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c984fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of text lengths\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Original text length distribution\n",
    "axes[0, 0].hist(df_processed['original_length'], bins=20, alpha=0.7, color='blue')\n",
    "axes[0, 0].set_title('Original Text Length Distribution')\n",
    "axes[0, 0].set_xlabel('Characters')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Cleaned text length distribution\n",
    "axes[0, 1].hist(df_processed['cleaned_length'], bins=20, alpha=0.7, color='green')\n",
    "axes[0, 1].set_title('Cleaned Text Length Distribution')\n",
    "axes[0, 1].set_xlabel('Characters')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Original word count distribution\n",
    "axes[1, 0].hist(df_processed['original_words'], bins=20, alpha=0.7, color='red')\n",
    "axes[1, 0].set_title('Original Word Count Distribution')\n",
    "axes[1, 0].set_xlabel('Words')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Cleaned word count distribution\n",
    "axes[1, 1].hist(df_processed['cleaned_words'], bins=20, alpha=0.7, color='orange')\n",
    "axes[1, 1].set_title('Cleaned Word Count Distribution')\n",
    "axes[1, 1].set_xlabel('Words')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ce8e2",
   "metadata": {},
   "source": [
    "## Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed dataset\n",
    "output_file = '../data/preprocessed_reviews.csv'\n",
    "\n",
    "# Select only necessary columns for next steps\n",
    "final_df = df_processed[['review', 'cleaned_review', 'sentiment']].copy()\n",
    "\n",
    "# Save to CSV\n",
    "final_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Preprocessed dataset saved to: {output_file}\")\n",
    "print(f\"Final dataset shape: {final_df.shape}\")\n",
    "print(f\"Columns: {final_df.columns.tolist()}\")\n",
    "\n",
    "# Display sample of final dataset\n",
    "print(\"\\nSample of preprocessed data:\")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b474c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of Step 2\n",
    "print(\"\\n=== STEP 2 COMPLETED ===\")\n",
    "print(\"✓ Text cleaning and preprocessing functions created\")\n",
    "print(\"✓ Applied lowercase conversion\")\n",
    "print(\"✓ Removed punctuation and special characters\")\n",
    "print(\"✓ Removed stopwords using NLTK\")\n",
    "print(\"✓ Applied stemming to reduce words to root forms\")\n",
    "print(\"✓ Generated text statistics and visualizations\")\n",
    "print(\"✓ Saved preprocessed dataset for next steps\")\n",
    "print(f\"✓ Dataset ready with {len(final_df)} cleaned reviews\")\n",
    "print(\"\\nNext: Feature extraction using TF-IDF vectorization\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
