{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455c6e17",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation\n",
    "\n",
    "This notebook covers training and evaluating sentiment analysis models:\n",
    "- Loading preprocessed features\n",
    "- Training Logistic Regression classifier\n",
    "- Training Naive Bayes classifier\n",
    "- Model evaluation and comparison\n",
    "- Performance metrics analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb7ad8",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d43a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec49ab",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5339ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original data for reference\n",
    "df = pd.read_csv('../data/preprocessed_reviews.csv')\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Load the TF-IDF vectorizer\n",
    "with open('../results/tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    tfidf_vectorizer = pickle.load(f)\n",
    "\n",
    "print(f\"‚úì TF-IDF vectorizer loaded\")\n",
    "print(f\"‚úì Vocabulary size: {len(tfidf_vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf93a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and labels\n",
    "X = tfidf_vectorizer.transform(df['cleaned_review'])\n",
    "y = df['sentiment'].values\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "\n",
    "# Split into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Train labels distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Test labels distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade8b69",
   "metadata": {},
   "source": [
    "## Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Logistic Regression\n",
    "print(\"Training Logistic Regression model...\")\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    C=1.0  # Regularization strength\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úì Logistic Regression model trained successfully!\")\n",
    "print(f\"‚úì Model coefficients shape: {lr_model.coef_.shape}\")\n",
    "print(f\"‚úì Model intercept: {lr_model.intercept_[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84dffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with Logistic Regression\n",
    "lr_train_pred = lr_model.predict(X_train)\n",
    "lr_test_pred = lr_model.predict(X_test)\n",
    "lr_test_proba = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate performance metrics\n",
    "lr_train_acc = accuracy_score(y_train, lr_train_pred)\n",
    "lr_test_acc = accuracy_score(y_test, lr_test_pred)\n",
    "lr_precision = precision_score(y_test, lr_test_pred)\n",
    "lr_recall = recall_score(y_test, lr_test_pred)\n",
    "lr_f1 = f1_score(y_test, lr_test_pred)\n",
    "lr_auc = roc_auc_score(y_test, lr_test_proba)\n",
    "\n",
    "print(\"Logistic Regression Performance:\")\n",
    "print(f\"Training Accuracy: {lr_train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {lr_test_acc:.4f}\")\n",
    "print(f\"Precision: {lr_precision:.4f}\")\n",
    "print(f\"Recall: {lr_recall:.4f}\")\n",
    "print(f\"F1-Score: {lr_f1:.4f}\")\n",
    "print(f\"AUC-ROC: {lr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f567a",
   "metadata": {},
   "source": [
    "## Train Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc7103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Multinomial Naive Bayes\n",
    "print(\"Training Naive Bayes model...\")\n",
    "\n",
    "nb_model = MultinomialNB(alpha=1.0)  # Laplace smoothing\n",
    "\n",
    "# Train the model\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úì Naive Bayes model trained successfully!\")\n",
    "print(f\"‚úì Model classes: {nb_model.classes_}\")\n",
    "print(f\"‚úì Class log priors: {nb_model.class_log_prior_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e4ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with Naive Bayes\n",
    "nb_train_pred = nb_model.predict(X_train)\n",
    "nb_test_pred = nb_model.predict(X_test)\n",
    "nb_test_proba = nb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate performance metrics\n",
    "nb_train_acc = accuracy_score(y_train, nb_train_pred)\n",
    "nb_test_acc = accuracy_score(y_test, nb_test_pred)\n",
    "nb_precision = precision_score(y_test, nb_test_pred)\n",
    "nb_recall = recall_score(y_test, nb_test_pred)\n",
    "nb_f1 = f1_score(y_test, nb_test_pred)\n",
    "nb_auc = roc_auc_score(y_test, nb_test_proba)\n",
    "\n",
    "print(\"Naive Bayes Performance:\")\n",
    "print(f\"Training Accuracy: {nb_train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {nb_test_acc:.4f}\")\n",
    "print(f\"Precision: {nb_precision:.4f}\")\n",
    "print(f\"Recall: {nb_recall:.4f}\")\n",
    "print(f\"F1-Score: {nb_f1:.4f}\")\n",
    "print(f\"AUC-ROC: {nb_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705fe7ac",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f59ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Naive Bayes'],\n",
    "    'Train Accuracy': [lr_train_acc, nb_train_acc],\n",
    "    'Test Accuracy': [lr_test_acc, nb_test_acc],\n",
    "    'Precision': [lr_precision, nb_precision],\n",
    "    'Recall': [lr_recall, nb_recall],\n",
    "    'F1-Score': [lr_f1, nb_f1],\n",
    "    'AUC-ROC': [lr_auc, nb_auc]\n",
    "})\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Determine best model\n",
    "best_model_idx = comparison_df['F1-Score'].idxmax()\n",
    "best_model = comparison_df.loc[best_model_idx, 'Model']\n",
    "print(f\"\\nBest performing model: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f247b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Performance metrics comparison\n",
    "metrics = ['Test Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//2, i%2]\n",
    "    bars = ax.bar(x_pos, comparison_df[metric], width)\n",
    "    ax.set_title(f'{metric} Comparison')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(comparison_df['Model'], rotation=45)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef042b23",
   "metadata": {},
   "source": [
    "## Confusion Matrix and Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0743edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Logistic Regression confusion matrix\n",
    "lr_cm = confusion_matrix(y_test, lr_test_pred)\n",
    "sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Logistic Regression\\nConfusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xticklabels(['Negative', 'Positive'])\n",
    "axes[0].set_yticklabels(['Negative', 'Positive'])\n",
    "\n",
    "# Naive Bayes confusion matrix\n",
    "nb_cm = confusion_matrix(y_test, nb_test_pred)\n",
    "sns.heatmap(nb_cm, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
    "axes[1].set_title('Naive Bayes\\nConfusion Matrix')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_xticklabels(['Negative', 'Positive'])\n",
    "axes[1].set_yticklabels(['Negative', 'Positive'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67abc097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification reports\n",
    "print(\"Logistic Regression - Classification Report:\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_test, lr_test_pred, target_names=['Negative', 'Positive']))\n",
    "\n",
    "print(\"\\nNaive Bayes - Classification Report:\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_test, nb_test_pred, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab90ff8",
   "metadata": {},
   "source": [
    "## ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f8a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Logistic Regression ROC\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_test_proba)\n",
    "plt.plot(lr_fpr, lr_tpr, label=f'Logistic Regression (AUC = {lr_auc:.3f})', linewidth=2)\n",
    "\n",
    "# Naive Bayes ROC\n",
    "nb_fpr, nb_tpr, _ = roc_curve(y_test, nb_test_proba)\n",
    "plt.plot(nb_fpr, nb_tpr, label=f'Naive Bayes (AUC = {nb_auc:.3f})', linewidth=2)\n",
    "\n",
    "# Random classifier line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', alpha=0.5)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Model Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc461491",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a80f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "print(\"Performing 5-fold cross-validation...\")\n",
    "\n",
    "# Combine train and test for full cross-validation\n",
    "X_full = X\n",
    "y_full = y\n",
    "\n",
    "# Logistic Regression CV\n",
    "lr_cv_scores = cross_val_score(lr_model, X_full, y_full, cv=5, scoring='accuracy')\n",
    "print(f\"\\nLogistic Regression CV Scores: {lr_cv_scores}\")\n",
    "print(f\"Mean CV Accuracy: {lr_cv_scores.mean():.4f} (+/- {lr_cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Naive Bayes CV\n",
    "nb_cv_scores = cross_val_score(nb_model, X_full, y_full, cv=5, scoring='accuracy')\n",
    "print(f\"\\nNaive Bayes CV Scores: {nb_cv_scores}\")\n",
    "print(f\"Mean CV Accuracy: {nb_cv_scores.mean():.4f} (+/- {nb_cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a9f16b",
   "metadata": {},
   "source": [
    "## Model Interpretation - Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa7af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Logistic Regression coefficients (feature importance)\n",
    "lr_coef = lr_model.coef_[0]\n",
    "\n",
    "# Get top positive and negative features\n",
    "top_positive_idx = np.argsort(lr_coef)[-10:]  # Top 10 positive\n",
    "top_negative_idx = np.argsort(lr_coef)[:10]   # Top 10 negative\n",
    "\n",
    "print(\"Top 10 Positive Sentiment Features (Logistic Regression):\")\n",
    "for idx in reversed(top_positive_idx):\n",
    "    print(f\"{feature_names[idx]}: {lr_coef[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 Negative Sentiment Features (Logistic Regression):\")\n",
    "for idx in top_negative_idx:\n",
    "    print(f\"{feature_names[idx]}: {lr_coef[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6644bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Top positive features\n",
    "pos_features = [feature_names[idx] for idx in reversed(top_positive_idx)]\n",
    "pos_coefs = [lr_coef[idx] for idx in reversed(top_positive_idx)]\n",
    "\n",
    "axes[0].barh(range(len(pos_features)), pos_coefs, color='green', alpha=0.7)\n",
    "axes[0].set_yticks(range(len(pos_features)))\n",
    "axes[0].set_yticklabels(pos_features)\n",
    "axes[0].set_xlabel('Coefficient Value')\n",
    "axes[0].set_title('Top Positive Sentiment Features')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Top negative features\n",
    "neg_features = [feature_names[idx] for idx in top_negative_idx]\n",
    "neg_coefs = [lr_coef[idx] for idx in top_negative_idx]\n",
    "\n",
    "axes[1].barh(range(len(neg_features)), neg_coefs, color='red', alpha=0.7)\n",
    "axes[1].set_yticks(range(len(neg_features)))\n",
    "axes[1].set_yticklabels(neg_features)\n",
    "axes[1].set_xlabel('Coefficient Value')\n",
    "axes[1].set_title('Top Negative Sentiment Features')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1e9ad8",
   "metadata": {},
   "source": [
    "## Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6448f2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained models\n",
    "models_dir = '../results'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save Logistic Regression model\n",
    "with open(os.path.join(models_dir, 'logistic_regression_model.pkl'), 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "\n",
    "# Save Naive Bayes model\n",
    "with open(os.path.join(models_dir, 'naive_bayes_model.pkl'), 'wb') as f:\n",
    "    pickle.dump(nb_model, f)\n",
    "\n",
    "# Save performance metrics\n",
    "results = {\n",
    "    'comparison_df': comparison_df,\n",
    "    'best_model': best_model,\n",
    "    'logistic_regression': {\n",
    "        'train_acc': lr_train_acc,\n",
    "        'test_acc': lr_test_acc,\n",
    "        'precision': lr_precision,\n",
    "        'recall': lr_recall,\n",
    "        'f1': lr_f1,\n",
    "        'auc': lr_auc,\n",
    "        'cv_scores': lr_cv_scores\n",
    "    },\n",
    "    'naive_bayes': {\n",
    "        'train_acc': nb_train_acc,\n",
    "        'test_acc': nb_test_acc,\n",
    "        'precision': nb_precision,\n",
    "        'recall': nb_recall,\n",
    "        'f1': nb_f1,\n",
    "        'auc': nb_auc,\n",
    "        'cv_scores': nb_cv_scores\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(models_dir, 'model_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "print(\"‚úì Models and results saved successfully!\")\n",
    "print(f\"‚úì Logistic Regression model saved\")\n",
    "print(f\"‚úì Naive Bayes model saved\")\n",
    "print(f\"‚úì Performance results saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79198f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of model training step\n",
    "print(\"\\n=== MODEL TRAINING COMPLETED ===\")\n",
    "print(\"‚úì Logistic Regression model trained and evaluated\")\n",
    "print(\"‚úì Naive Bayes model trained and evaluated\")\n",
    "print(\"‚úì Model comparison performed\")\n",
    "print(\"‚úì Cross-validation completed\")\n",
    "print(\"‚úì Feature importance analysis done\")\n",
    "print(\"‚úì All models and results saved\")\n",
    "print(f\"\\nüèÜ Best performing model: {best_model}\")\n",
    "print(f\"üéØ Best F1-Score: {comparison_df['F1-Score'].max():.4f}\")\n",
    "print(\"\\nNext: Visualization and word clouds for bonus features\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
