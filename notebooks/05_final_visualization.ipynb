{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e17f1be",
   "metadata": {},
   "source": [
    "# Step 5: Final Visualization and Model Comparison\n",
    "\n",
    "This notebook completes the sentiment analysis project with visualizations and bonus features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c17697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813f5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and models\n",
    "df = pd.read_csv('../data/preprocessed_reviews.csv')\n",
    "\n",
    "with open('../results/tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "with open('../results/logistic_regression_model.pkl', 'rb') as f:\n",
    "    lr_model = pickle.load(f)\n",
    "with open('../results/naive_bayes_model.pkl', 'rb') as f:\n",
    "    nb_model = pickle.load(f)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db59b6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for evaluation\n",
    "X = vectorizer.transform(df['cleaned_review'])\n",
    "y = df['sentiment'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "nb_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracies\n",
    "lr_acc = accuracy_score(y_test, lr_pred)\n",
    "nb_acc = accuracy_score(y_test, nb_pred)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {lr_acc:.4f}\")\n",
    "print(f\"Naive Bayes Accuracy: {nb_acc:.4f}\")\n",
    "print(f\"\\nBest Model: {'Logistic Regression' if lr_acc > nb_acc else 'Naive Bayes'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dec85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison Visualization\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "models = ['Logistic Regression', 'Naive Bayes']\n",
    "accuracies = [lr_acc, nb_acc]\n",
    "\n",
    "bars = ax.bar(models, accuracies, color=['blue', 'orange'], alpha=0.7)\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "            f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd15a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Word Clouds\n",
    "positive_text = ' '.join(df[df['sentiment'] == 1]['cleaned_review'])\n",
    "negative_text = ' '.join(df[df['sentiment'] == 0]['cleaned_review'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Positive word cloud\n",
    "positive_wc = WordCloud(width=800, height=400, background_color='white',\n",
    "                       colormap='Greens', max_words=100, random_state=42).generate(positive_text)\n",
    "axes[0].imshow(positive_wc, interpolation='bilinear')\n",
    "axes[0].set_title('Positive Sentiment Word Cloud', fontsize=16, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Negative word cloud\n",
    "negative_wc = WordCloud(width=800, height=400, background_color='white',\n",
    "                       colormap='Reds', max_words=100, random_state=42).generate(negative_text)\n",
    "axes[1].imshow(negative_wc, interpolation='bilinear')\n",
    "axes[1].set_title('Negative Sentiment Word Cloud', fontsize=16, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save word clouds\n",
    "positive_wc.to_file('../results/positive_wordcloud.png')\n",
    "negative_wc.to_file('../results/negative_wordcloud.png')\n",
    "print(\"Word clouds saved to results/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde9bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Frequent Words Analysis\n",
    "pos_words = Counter(positive_text.split()).most_common(15)\n",
    "neg_words = Counter(negative_text.split()).most_common(15)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Positive words\n",
    "words, counts = zip(*pos_words)\n",
    "axes[0].barh(range(len(words)), counts, color='green', alpha=0.7)\n",
    "axes[0].set_yticks(range(len(words)))\n",
    "axes[0].set_yticklabels(words)\n",
    "axes[0].set_title('Top 15 Most Frequent Words in Positive Reviews')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Negative words\n",
    "words, counts = zip(*neg_words)\n",
    "axes[1].barh(range(len(words)), counts, color='red', alpha=0.7)\n",
    "axes[1].set_yticks(range(len(words)))\n",
    "axes[1].set_yticklabels(words)\n",
    "axes[1].set_title('Top 15 Most Frequent Words in Negative Reviews')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 5 Positive Words:\", [w[0] for w in pos_words[:5]])\n",
    "print(\"Top 5 Negative Words:\", [w[0] for w in neg_words[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9398fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance from Logistic Regression\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefficients = lr_model.coef_[0]\n",
    "\n",
    "# Top positive and negative features\n",
    "top_pos_idx = np.argsort(coefficients)[-10:]\n",
    "top_neg_idx = np.argsort(coefficients)[:10]\n",
    "\n",
    "top_pos_features = [(feature_names[i], coefficients[i]) for i in reversed(top_pos_idx)]\n",
    "top_neg_features = [(feature_names[i], coefficients[i]) for i in top_neg_idx]\n",
    "\n",
    "print(\"Top 10 Positive Features:\")\n",
    "for feature, coef in top_pos_features:\n",
    "    print(f\"  {feature}: {coef:.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 Negative Features:\")\n",
    "for feature, coef in top_neg_features:\n",
    "    print(f\"  {feature}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cdf493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Project Summary\n",
    "print(\"=\" * 50)\n",
    "print(\"     SENTIMENT ANALYSIS PROJECT COMPLETED\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüìä Dataset: {len(df)} reviews ({len(df[df['sentiment']==1])} positive, {len(df[df['sentiment']==0])} negative)\")\n",
    "print(f\"\\nü§ñ Models Trained:\")\n",
    "print(f\"   ‚Ä¢ Logistic Regression: {lr_acc:.4f} accuracy\")\n",
    "print(f\"   ‚Ä¢ Naive Bayes: {nb_acc:.4f} accuracy\")\n",
    "\n",
    "best_model = 'Logistic Regression' if lr_acc > nb_acc else 'Naive Bayes'\n",
    "print(f\"\\nüèÜ Best Model: {best_model}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Deliverables:\")\n",
    "print(f\"   ‚Ä¢ Data preprocessing pipeline\")\n",
    "print(f\"   ‚Ä¢ TF-IDF feature extraction\")\n",
    "print(f\"   ‚Ä¢ Model training and evaluation\")\n",
    "print(f\"   ‚Ä¢ Word clouds and visualizations\")\n",
    "print(f\"   ‚Ä¢ Feature importance analysis\")\n",
    "\n",
    "print(f\"\\nüéâ PROJECT SUCCESSFULLY COMPLETED!\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
